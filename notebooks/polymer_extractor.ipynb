{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2bdd82",
   "metadata": {},
   "source": [
    "# Polymer NLP Extractor - Complete Pipeline Notebook\n",
    "\n",
    "This notebook contains the complete end-to-end pipeline for the Polymer NLP Extractor system, adapted for Google Colab.\n",
    "\n",
    "## ðŸ“‹ **Notebook Flow**\n",
    "\n",
    "### **Main Pipeline (Run in Order 1-11):**\n",
    "1. **Setup & Dependencies** - Import GROBID, install requirements\n",
    "2. **Backend Initialization** - Setup Appwrite services and environment  \n",
    "3. **GROBID Service** - Start and configure GROBID server\n",
    "4. **File Processing** - Process PDF â†’ TEI XML â†’ Token Packing\n",
    "5. **Ensemble Inference Service** - Load fine-tuned models and run inference\n",
    "6. **Model Verification** - Check if fine-tuned models exist\n",
    "7. **Evaluation Service** - Compare predictions against ground truth\n",
    "8. **Run Inference** - Execute ensemble inference on processed file\n",
    "9. **Run Evaluation** - Evaluate inference results\n",
    "10. **Export Models** - Zip fine-tuned models\n",
    "11. **Save to Drive** - Mount and copy models to Google Drive\n",
    "\n",
    "### **Independent Training (Run Separately):**\n",
    "12. **Fine-tuning Pipeline** - Train ensemble models (run this first if models don't exist)\n",
    "\n",
    "## âš ï¸ **Important Notes**\n",
    "\n",
    "- **Services are identical to API versions** - Only class names changed (e.g., `ColabEnsembleInferenceService` vs `EnsembleInferenceService`)\n",
    "- **Logic is unchanged** - All core algorithms, thresholds, and processing steps are identical\n",
    "- **Run fine-tuning first** if you don't have models, otherwise skip to main pipeline\n",
    "- **Models are saved to** `workspace/models/finetuned/` and automatically detected\n",
    "\n",
    "## ðŸ”„ **Execution Order**\n",
    "\n",
    "**If you have no models:** Run cell 12 (Fine-tuning) first, then cells 1-11  \n",
    "**If you have models:** Run cells 1-11 directly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e70e6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# add root directory of the project to python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/content/polymer_nlp_extractor')\n",
    "PROJECT_ROOT = '/content/polymer_nlp_extractor'"
   ]
  },
  {
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy finetuned models from Google Drive to workspace\n",
    "!cp /content/drive/MyDrive/polymer_nlp_extractor/finetuned_models.zip {PROJECT_ROOT}/\n",
    "\n",
    "# Extract to correct models directory structure\n",
    "!cd {PROJECT_ROOT} && unzip -o finetuned_models.zip && rm finetuned_models.zip\n",
    "\n",
    "# Verify the models are in the correct location\n",
    "!ls -la {PROJECT_ROOT}/workspace/models/finetuned/"
   ],
   "id": "df354694"
  },
  {
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import grobid\n",
    "!wget https://github.com/kermitt2/grobid/archive/0.8.2.zip\n",
    "!unzip 0.8.2.zip -d /content/polymer_nlp_extractor/workspace\n",
    "!rm 0.8.2.zip"
   ],
   "id": "2a408111"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2e7e7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install dependencies with logs\n",
    "! pip install -r {PROJECT_ROOT}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable\n",
    "RESET_BACKEND = False\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_DIR = PROJECT_ROOT + \"/.env\"\n",
    "\n",
    "from polymer_extractor.services.setup_service import SetupService\n",
    "from fastapi import HTTPException\n",
    "\n",
    "# if .env file does not exist, exit with error\n",
    "if not os.path.exists(ENV_DIR):\n",
    "    raise FileNotFoundError(\n",
    "        f\".env file not found at {ENV_DIR}. Please create it with the necessary environment variables.\")\n",
    "# Load environment variables\n",
    "load_dotenv(ENV_DIR)\n",
    "\n",
    "setup_service = SetupService()\n",
    "\n",
    "if RESET_BACKEND:\n",
    "    try:\n",
    "        # Get database and bucket managers from setup service\n",
    "        db_manager = setup_service.get_database_manager()\n",
    "        bucket_manager = setup_service.get_bucket_manager()\n",
    "\n",
    "        # Delete collections\n",
    "        collections = db_manager.list_collections()\n",
    "        for col in collections:\n",
    "            if col['$id'] != \"system_logs\":  # Skip Logger's autonomous collection\n",
    "                db_manager.delete_collection(col['$id'])\n",
    "\n",
    "        # Delete buckets\n",
    "        buckets = bucket_manager.list_buckets()\n",
    "        for bkt in buckets:\n",
    "            bucket_manager.delete_bucket(bkt['$id'])\n",
    "\n",
    "        # Reinitialize\n",
    "        setup_service.initialize_all_resources()\n",
    "        print(\"System reset and reinitialized successfully.\")\n",
    "        result = {\"status\": \"success\", \"message\": \"System reset and reinitialized successfully.\"}\n",
    "        print(f\"Reset completed: {result['message']}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to reset system: {str(e)}\"\n",
    "        print(f\"ERROR: Failed to reset system: {str(e)}\")\n",
    "        print(f\"Reset failed: {error_msg}\")\n",
    "        raise HTTPException(status_code=500, detail=error_msg)\n",
    "else:\n",
    "    try:\n",
    "        setup_service.initialize_all_resources()\n",
    "        print(\"System initialized successfully.\")\n",
    "        print(\"System initialization completed successfully.\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to initialize system: {str(e)}\"\n",
    "        print(f\"ERROR: Failed to initialize system: {str(e)}\")\n",
    "        print(f\"Initialization failed: {error_msg}\")\n",
    "        raise HTTPException(status_code=500, detail=error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from polymer_extractor.services import grobid_service\n",
    "from polymer_extractor.services.grobid_service import GrobidService\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# === Response Models ===\n",
    "\n",
    "class ServerStatusResponse(BaseModel):\n",
    "    \"\"\"Response model for server status checks.\"\"\"\n",
    "    status: str\n",
    "    message: str\n",
    "    server_url: str\n",
    "\n",
    "# Define workspace directory if not already defined\n",
    "WORKSPACE_DIR = os.path.join(PROJECT_ROOT, \"workspace\")\n",
    "\n",
    "# Initialize GrobidService\n",
    "grobid_service = GrobidService()\n",
    "\n",
    "# Check if Grobid is running\n",
    "try:\n",
    "    grobid_service.check_server_status()\n",
    "    grobid_is_running = True\n",
    "    grobid_status_response = ServerStatusResponse(\n",
    "        status=\"running\",\n",
    "        message=\"GROBID server is alive and responding\",\n",
    "        server_url=grobid_service.grobid_server_url\n",
    "    )\n",
    "    print(\"GROBID server is alive and responding\")\n",
    "except Exception as e:\n",
    "    grobid_is_running = False\n",
    "    print(f\"WARNING: GROBID server status check failed: {e}\")\n",
    "    grobid_status_response = ServerStatusResponse(\n",
    "        status=\"unreachable\",\n",
    "        message=f\"GROBID server is not responding: {str(e)}\",\n",
    "        server_url=grobid_service.grobid_server_url\n",
    "    )\n",
    "\n",
    "# If Grobid is not running, start the server\n",
    "if not grobid_is_running:\n",
    "    try:\n",
    "        grobid_home = os.path.join(WORKSPACE_DIR, \"grobid-0.8.2\")\n",
    "        print(\"Starting GROBID server via API request\")\n",
    "        grobid_service.start_server(grobid_home=grobid_home)\n",
    "        grobid_status_response = ServerStatusResponse(\n",
    "            status=\"started\",\n",
    "            message=\"GROBID server started successfully\",\n",
    "            server_url=grobid_service.grobid_server_url\n",
    "        )\n",
    "        print(\"GROBID server started successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to start GROBID server via API: {e}\")\n",
    "        raise Exception(f\"Failed to start GROBID server: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a File\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from fastapi import HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# === Import real services ===\n",
    "from polymer_extractor.services.grobid_service import GrobidService\n",
    "from polymer_extractor.services.tei_processing_service import TEIProcessingService\n",
    "from polymer_extractor.services.tokenizer_service import TokenizerService\n",
    "from polymer_extractor.services.token_packing_service import TokenPackingService\n",
    "\n",
    "# Request models\n",
    "class TokenPackRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Request body for /api/preprocess/tokenpack\n",
    "    \"\"\"\n",
    "    tei_path: str = Field(\n",
    "        ...,\n",
    "        description=\"Absolute filesystem path to cleaned TEI XML file.\"\n",
    "    )\n",
    "\n",
    "class TEIProcessRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Request body for /api/preprocess/tei\n",
    "    \"\"\"\n",
    "    tei_path: str = Field(\n",
    "        ...,\n",
    "        description=\"Absolute filesystem path to the TEI XML file for cleaning and metadata extraction.\"\n",
    "    )\n",
    "\n",
    "class ProcessingResult(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "    original_file: str\n",
    "    pdf_file: str = None\n",
    "    metadata: dict = {}\n",
    "    local_tei_path: str = None\n",
    "    storage_success: bool = False\n",
    "    storage_errors: list = []\n",
    "\n",
    "def cleanup_temp_file(temp_path):\n",
    "    if temp_path and os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "\n",
    "# Initialize services and variables\n",
    "WORKSPACE_DIR = os.path.join(PROJECT_ROOT, \"workspace\")\n",
    "# grobid_service = MockGrobidService()\n",
    "grobid_service = GrobidService()\n",
    "\n",
    "\n",
    "# Assuming a File class similar to FastAPI's UploadFile is needed for process_document\n",
    "# Creating a simple mock class that provides the necessary attributes (filename, read)\n",
    "class FileLike:\n",
    "    def __init__(self, filepath):\n",
    "        self.filename = os.path.basename(filepath)\n",
    "        self.filepath = filepath\n",
    "\n",
    "    async def read(self):\n",
    "        with open(self.filepath, 'rb') as f:\n",
    "            return f.read()\n",
    "\n",
    "\n",
    "async def process_file_pipeline():\n",
    "    \"\"\"Main processing pipeline for a file\"\"\"\n",
    "\n",
    "    file_path = os.path.join(WORKSPACE_DIR, \"raw_inputs\", \"057.pdf\")  # Example file path\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise HTTPException(status_code=404, detail=f\"File not found: {file_path}\")\n",
    "\n",
    "    # file = MockFile(file_path)\n",
    "    file = FileLike(file_path)\n",
    "\n",
    "    # Step 1: Upload a file via grobid and retain its path for step 2\n",
    "    temp_path = None\n",
    "    try:\n",
    "        allowed_extensions = {'.pdf', '.xml', '.html', '.htm'}\n",
    "        file_ext = Path(file.filename).suffix.lower()\n",
    "        if file_ext not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}\"\n",
    "            )\n",
    "\n",
    "        original_stem = Path(file.filename).stem\n",
    "        temp_dir = Path(tempfile.gettempdir())\n",
    "        temp_filename = f\"{original_stem}_{int(time.time())}{file_ext}\"\n",
    "        temp_path = temp_dir / temp_filename\n",
    "\n",
    "        with open(temp_path, 'wb') as temp_file:\n",
    "            content = await file.read()\n",
    "            temp_file.write(content)\n",
    "\n",
    "        result = grobid_service.process_document(temp_path) # Assuming process_document takes a file path\n",
    "\n",
    "\n",
    "        grobid_result = ProcessingResult(\n",
    "            success=True,\n",
    "            message=f\"Successfully processed {file.filename}\",\n",
    "            original_file=file.filename,\n",
    "            pdf_file=result.get('pdf_file'),\n",
    "            metadata=result.get('metadata', {}),\n",
    "            local_tei_path=result.get('local_tei_path'),\n",
    "            storage_success=result.get('storage_success', False),\n",
    "            storage_errors=result.get('storage_errors', [])\n",
    "        )\n",
    "\n",
    "        tei_path = result.get('local_tei_path')\n",
    "\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Processing failed for uploaded file - {e}\")\n",
    "        if temp_path and temp_path.exists():\n",
    "            cleanup_temp_file(temp_path)\n",
    "        raise HTTPException(status_code=500, detail=f\"Processing failed: {str(e)}\")\n",
    "\n",
    "    # Step 2: Take the grobid output path as input and perform processing of the xml\n",
    "    if not tei_path:\n",
    "        raise HTTPException(status_code=500, detail=\"No TEI path from grobid processing\")\n",
    "\n",
    "    req_tei = TEIProcessRequest(tei_path=tei_path)\n",
    "\n",
    "    print(f\"INFO: Received TEI processing request: tei_path={req_tei.tei_path}\")\n",
    "\n",
    "    if not os.path.isabs(req_tei.tei_path) or not os.path.exists(req_tei.tei_path):\n",
    "        print(f\"ERROR: TEI file not found: {req_tei.tei_path}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"TEI file not found: {req_tei.tei_path}\")\n",
    "\n",
    "    try:\n",
    "        # service = MockTEIProcessingService()\n",
    "        service = TEIProcessingService()\n",
    "        tei_result = service.process(req_tei.tei_path)\n",
    "        processed_tei_path = tei_result.get('processed_path', req_tei.tei_path)\n",
    "\n",
    "        print(f\"INFO: TEI processing completed for {req_tei.tei_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: TEI processing failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"TEI processing failed: {str(e)}\")\n",
    "\n",
    "    # Step 3: Attempt tokenization\n",
    "    force = False  # Define force parameter\n",
    "    print(f\"INFO: Received tokenizer audit request with force={force}\")\n",
    "    try:\n",
    "        # tokenizer_service = MockTokenizerService()\n",
    "        tokenizer_service = TokenizerService()\n",
    "        audit_results = tokenizer_service.audit_and_extend_all(force=force)\n",
    "\n",
    "        print(\"INFO: Tokenizer audit completed successfully.\")\n",
    "\n",
    "        tokenizer_result = {\n",
    "            \"success\": True,\n",
    "            \"message\": \"All tokenizers audited and extended successfully.\",\n",
    "            \"force_rebuild\": force,\n",
    "            \"audit_results\": audit_results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Tokenizer audit failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Tokenizer audit failed: {str(e)}\")\n",
    "\n",
    "    # Step 4: Take the output path of the xml processing in step 2 and use it for token packing\n",
    "    req_token = TokenPackRequest(tei_path=processed_tei_path)\n",
    "\n",
    "    print(f\"INFO: Received token packing request: tei_path='{req_token.tei_path}'\")\n",
    "\n",
    "    if not os.path.isfile(req_token.tei_path):\n",
    "        print(f\"ERROR: TEI file does not exist: {req_token.tei_path}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"TEI file not found: {req_token.tei_path}\")\n",
    "\n",
    "    try:\n",
    "        # service = MockTokenPackingService()\n",
    "        service = TokenPackingService()\n",
    "        token_result = service.process(tei_path=req_token.tei_path)\n",
    "\n",
    "        print(f\"INFO: Token packing completed for all models on {req_token.tei_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Token packing failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Token packing failed: {str(e)}\")\n",
    "\n",
    "    # Clean up temp file\n",
    "    if temp_path and temp_path.exists():\n",
    "        cleanup_temp_file(temp_path)\n",
    "\n",
    "    # Return combined results\n",
    "    return {\n",
    "        \"grobid_result\": grobid_result,\n",
    "        \"tei_result\": tei_result,\n",
    "        \"tokenizer_result\": tokenizer_result,\n",
    "        \"token_result\": token_result\n",
    "    }\n",
    "\n",
    "# Colab is already asynced so all async functions can be called directly via await\n",
    "result = await process_file_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96db078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Inference Service (Colab Version)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "from polymer_extractor.model_config import (\n",
    "    ENSEMBLE_MODELS,\n",
    "    LABELS,\n",
    "    LABEL2ID,\n",
    "    ID2LABEL,\n",
    "    get_entity_threshold\n",
    ")\n",
    "from polymer_extractor.services.constants.property_table import PROPERTY_TABLE\n",
    "from polymer_extractor.services.token_packing_service import TokenPackingService\n",
    "from polymer_extractor.storage.database_manager import DatabaseManager\n",
    "from polymer_extractor.utils.logging import Logger\n",
    "\n",
    "# Initialize services\n",
    "logger = Logger()\n",
    "db = DatabaseManager()\n",
    "\n",
    "STOPWORDS = {\"of\", \"the\", \"at\", \"in\", \"to\", \"for\", \"on\"}\n",
    "REMOVE_SUFFIXES = [\" based\", \" derived\", \" containing\"]\n",
    "\n",
    "class ColabEnsembleInferenceService:\n",
    "    def __init__(self):\n",
    "        self.models_cfg = ENSEMBLE_MODELS\n",
    "        self.models_dir = Path(WORKSPACE_DIR) / \"models\" / \"finetuned\"\n",
    "        self.results_dir = Path(WORKSPACE_DIR) / \"exports\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def run_inference(self, tei_path: str) -> Dict[str, Any]:\n",
    "        logger.info(f\"[EnsembleInference] Starting pipeline for {tei_path}\",\n",
    "                    source=\"ColabEnsembleInferenceService.run_inference\")\n",
    "\n",
    "        base_name = Path(tei_path).stem\n",
    "        all_predictions = []\n",
    "\n",
    "        packing_service = TokenPackingService()\n",
    "        packing_result = packing_service.process(tei_path)\n",
    "        windows = None\n",
    "\n",
    "        for model_cfg in self.models_cfg:\n",
    "            model_name = model_cfg.name\n",
    "            model_path = self.models_dir / model_name\n",
    "\n",
    "            tokenizer_path = Path(WORKSPACE_DIR) / \"models\" / \"tokenizers\" / f\"{model_name}_extended\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                tokenizer_path if tokenizer_path.exists() else model_cfg.model_id,\n",
    "                use_fast=True\n",
    "            )\n",
    "\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_path,\n",
    "                num_labels=len(LABELS),\n",
    "                id2label=ID2LABEL,\n",
    "                label2id=LABEL2ID\n",
    "            ).eval()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model.cuda()\n",
    "\n",
    "            if windows is None:\n",
    "                windows_path = packing_result[\"models_processed\"][model_name][\"windows_file\"]\n",
    "                with open(windows_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    windows = json.load(f)\n",
    "\n",
    "            preds = self._infer_model(model, tokenizer, windows, model_name)\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        merged_preds = self._merge_predictions(all_predictions)\n",
    "        final_results = self._ensemble_vote_and_postprocess(merged_preds)\n",
    "\n",
    "        self._save_results(final_results, base_name)\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"tei_file\": tei_path,\n",
    "            \"models_used\": [m.name for m in self.models_cfg],\n",
    "            \"num_entities\": sum(len(v) for v in final_results.values()),\n",
    "            \"output_file\": str(self.results_dir / f\"{base_name}_ensemble_results.json\")\n",
    "        }\n",
    "\n",
    "    def _infer_model(self, model, tokenizer, windows, model_name: str) -> List[Dict[str, Any]]:\n",
    "        predictions = []\n",
    "        for win in windows:\n",
    "            inputs = {\n",
    "                \"input_ids\": torch.tensor([win[\"input_ids\"]]),\n",
    "                \"attention_mask\": torch.tensor([win[\"attention_mask\"]])\n",
    "            }\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                probs = softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "                pred_ids = np.argmax(probs, axis=-1)\n",
    "\n",
    "            for idx, label_id in enumerate(pred_ids):\n",
    "                if ID2LABEL[label_id] == \"O\":\n",
    "                    continue\n",
    "\n",
    "                offset = win[\"offset_mapping\"][idx]\n",
    "                if offset[0] == offset[1]:\n",
    "                    continue\n",
    "\n",
    "                predictions.append({\n",
    "                    \"label\": ID2LABEL[label_id],\n",
    "                    \"text\": win[\"text\"][offset[0]:offset[1]],\n",
    "                    \"char_start\": offset[0],\n",
    "                    \"char_end\": offset[1],\n",
    "                    \"confidence\": float(probs[idx][label_id]),\n",
    "                    \"model\": model_name\n",
    "                })\n",
    "        return predictions\n",
    "\n",
    "    def _merge_predictions(self, all_predictions: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "        return [p for preds in all_predictions for p in preds]\n",
    "\n",
    "    def _ensemble_vote_and_postprocess(self, predictions: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        clustered = defaultdict(list)\n",
    "        for pred in predictions:\n",
    "            entity_type = pred[\"label\"].split(\"-\")[-1]\n",
    "            clustered[entity_type].append(pred)\n",
    "\n",
    "        final_results = defaultdict(list)\n",
    "        for entity_type, preds in clustered.items():\n",
    "            preds.sort(key=lambda x: x[\"char_start\"])\n",
    "\n",
    "            while preds:\n",
    "                cluster = [preds.pop(0)]\n",
    "                i = 0\n",
    "                while i < len(preds):\n",
    "                    if preds[i][\"char_start\"] <= cluster[-1][\"char_end\"]:\n",
    "                        cluster.append(preds.pop(i))\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "                conf_scores = []\n",
    "                for c in cluster:\n",
    "                    model_cfg = next(m for m in ENSEMBLE_MODELS if m.name == c[\"model\"])\n",
    "                    conf_scores.append(c[\"confidence\"] * model_cfg.get_dynamic_weight(entity_type))\n",
    "\n",
    "                avg_conf = np.mean(conf_scores)\n",
    "                threshold = get_entity_threshold(entity_type, [], \"simple_majority\", conf_scores)\n",
    "\n",
    "                if avg_conf >= threshold:\n",
    "                    best_span = max(cluster, key=lambda x: x[\"confidence\"])\n",
    "                    cleaned_text = self._clean_span(best_span[\"text\"])\n",
    "                    if cleaned_text:\n",
    "                        final_results[entity_type].append({\n",
    "                            \"text\": cleaned_text,\n",
    "                            \"char_start\": best_span[\"char_start\"],\n",
    "                            \"char_end\": best_span[\"char_end\"],\n",
    "                            \"confidence\": round(avg_conf, 4),\n",
    "                            \"models_voted\": [c[\"model\"] for c in cluster]\n",
    "                        })\n",
    "\n",
    "        return dict(final_results)\n",
    "\n",
    "    def _clean_span(self, text: str) -> str:\n",
    "        \"\"\"Fix token joins, remove suffixes, and trim stopwords.\"\"\"\n",
    "        text = text.replace(\"##\", \"\")\n",
    "        text = re.sub(r\"(?<=[a-z])(?=[A-Z])\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # Remove unwanted suffixes if not canonical\n",
    "        for suffix in REMOVE_SUFFIXES:\n",
    "            if text.lower().endswith(suffix):\n",
    "                base = text[: -len(suffix)].strip()\n",
    "                if base.lower() not in [p[\"property\"].lower() for p in PROPERTY_TABLE]:\n",
    "                    text = base\n",
    "\n",
    "        # Remove trailing stopwords\n",
    "        parts = text.split()\n",
    "        while parts and parts[-1].lower() in STOPWORDS:\n",
    "            parts.pop()\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    def _save_results(self, results: Dict[str, Any], base_name: str):\n",
    "        local_path = self.results_dir / f\"{base_name}_ensemble_results.json\"\n",
    "        with open(local_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        json_str = json.dumps(results, ensure_ascii=False)\n",
    "        truncated = json_str[:500000]\n",
    "\n",
    "        try:\n",
    "            db.create_document(\"extraction_metadata\", {\n",
    "                \"file_name\": base_name,\n",
    "                \"extracted_entities\": truncated,\n",
    "                \"full_json_path\": str(local_path)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Appwrite save failed: {e}\",\n",
    "                         source=\"ColabEnsembleInferenceService._save_results\", error=e)\n",
    "\n",
    "# Initialize the ensemble inference service\n",
    "ensemble_service = ColabEnsembleInferenceService()\n",
    "print(\"[INFO] Ensemble Inference Service initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b49f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Fine-tuned Models Availability\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from polymer_extractor.model_config import ENSEMBLE_MODELS\n",
    "\n",
    "def verify_models_exist():\n",
    "    \"\"\"\n",
    "    Verify that all fine-tuned models exist in the expected directory.\n",
    "    \"\"\"\n",
    "    models_dir = Path(WORKSPACE_DIR) / \"models\" / \"finetuned\"\n",
    "    tokenizers_dir = Path(WORKSPACE_DIR) / \"models\" / \"tokenizers\"\n",
    "    \n",
    "    print(f\"[INFO] Checking for fine-tuned models in: {models_dir}\")\n",
    "    print(f\"[INFO] Checking for extended tokenizers in: {tokenizers_dir}\")\n",
    "    \n",
    "    # Check if models directory exists\n",
    "    if not models_dir.exists():\n",
    "        print(f\"[WARNING] Models directory does not exist: {models_dir}\")\n",
    "        return False\n",
    "        \n",
    "    missing_models = []\n",
    "    available_models = []\n",
    "    \n",
    "    for model_cfg in ENSEMBLE_MODELS:\n",
    "        model_path = models_dir / model_cfg.name\n",
    "        tokenizer_path = tokenizers_dir / f\"{model_cfg.name}_extended\"\n",
    "        \n",
    "        # Check for required model files\n",
    "        required_files = [\"config.json\", \"pytorch_model.bin\"]\n",
    "        model_exists = all((model_path / file).exists() for file in required_files)\n",
    "        \n",
    "        if model_exists:\n",
    "            available_models.append({\n",
    "                \"name\": model_cfg.name,\n",
    "                \"model_id\": model_cfg.model_id,\n",
    "                \"path\": str(model_path),\n",
    "                \"has_extended_tokenizer\": tokenizer_path.exists(),\n",
    "                \"tokenizer_path\": str(tokenizer_path) if tokenizer_path.exists() else \"Using base tokenizer\"\n",
    "            })\n",
    "            print(f\"âœ… {model_cfg.name}: Found fine-tuned model\")\n",
    "            if tokenizer_path.exists():\n",
    "                print(f\"   â””â”€â”€ Extended tokenizer: Available\")\n",
    "            else:\n",
    "                print(f\"   â””â”€â”€ Extended tokenizer: Will use base model tokenizer\")\n",
    "        else:\n",
    "            missing_models.append({\n",
    "                \"name\": model_cfg.name,\n",
    "                \"model_id\": model_cfg.model_id,\n",
    "                \"expected_path\": str(model_path)\n",
    "            })\n",
    "            print(f\"âŒ {model_cfg.name}: Missing fine-tuned model at {model_path}\")\n",
    "    \n",
    "    print(f\"\\n[SUMMARY]\")\n",
    "    print(f\"Available models: {len(available_models)}/{len(ENSEMBLE_MODELS)}\")\n",
    "    print(f\"Missing models: {len(missing_models)}\")\n",
    "    \n",
    "    if missing_models:\n",
    "        print(f\"\\n[ACTION REQUIRED]\")\n",
    "        print(f\"The following models need to be fine-tuned first:\")\n",
    "        for model in missing_models:\n",
    "            print(f\"  - {model['name']} ({model['model_id']})\")\n",
    "        print(f\"\\nðŸ’¡ Run the fine-tuning cell (cell 7) first to create these models.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"\\nâœ… All models are available for ensemble inference!\")\n",
    "        return True\n",
    "\n",
    "# Run verification\n",
    "models_ready = verify_models_exist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural Identity Verification\n",
    "\n",
    "\"\"\"\n",
    "VERIFICATION: Colab Services vs API Services\n",
    "\n",
    "This cell confirms that the Colab services (ColabEnsembleInferenceService and ColabEvaluationService) \n",
    "are structurally identical to their API counterparts, with only minimal changes for Colab compatibility.\n",
    "\n",
    "ENSEMBLE INFERENCE SERVICE COMPARISON:\n",
    "=====================================\n",
    "API: EnsembleInferenceService  |  Colab: ColabEnsembleInferenceService\n",
    "\n",
    "âœ… IDENTICAL CORE METHODS:\n",
    "- __init__(): Same path configurations\n",
    "- run_inference(): Identical main pipeline logic  \n",
    "- _infer_model(): Same model inference logic\n",
    "- _merge_predictions(): Identical prediction merging\n",
    "- _ensemble_vote_and_postprocess(): Same ensemble voting algorithm\n",
    "- _clean_span(): Identical text cleaning logic\n",
    "- _save_results(): Same local + Appwrite saving\n",
    "\n",
    "âœ… IDENTICAL CONFIGURATIONS:\n",
    "- models_dir: Path(WORKSPACE_DIR) / \"models\" / \"finetuned\" \n",
    "- results_dir: Path(WORKSPACE_DIR) / \"exports\"\n",
    "- STOPWORDS: {\"of\", \"the\", \"at\", \"in\", \"to\", \"for\", \"on\"}\n",
    "- REMOVE_SUFFIXES: [\" based\", \" derived\", \" containing\"]\n",
    "\n",
    "âœ… IDENTICAL ALGORITHMS:\n",
    "- Token packing service integration\n",
    "- Multi-model inference loop\n",
    "- Confidence-weighted ensemble voting\n",
    "- Dynamic thresholding via get_entity_threshold()\n",
    "- Span overlap clustering\n",
    "- Text post-processing and cleaning\n",
    "\n",
    "EVALUATION SERVICE COMPARISON:\n",
    "===============================\n",
    "API: EvaluationService  |  Colab: ColabEvaluationService\n",
    "\n",
    "âœ… IDENTICAL CORE METHODS:\n",
    "- __init__(): Same collection and path setup\n",
    "- evaluate(): Identical evaluation pipeline\n",
    "- _find_matching_dataset(): Same fuzzy matching logic\n",
    "- _download_dataset(): Identical dataset download\n",
    "- _load_predictions(): Same local + Appwrite fallback\n",
    "- _normalize_groundtruth(): Identical entity extraction\n",
    "- _normalize_predictions(): Same prediction formatting\n",
    "- _compute_metrics(): Identical fuzzy span matching + metrics\n",
    "- _save_to_metadata(): Same metadata and CSV saving\n",
    "\n",
    "âœ… IDENTICAL METRICS CALCULATION:\n",
    "- difflib.SequenceMatcher for fuzzy matching\n",
    "- Precision, Recall, F1, Accuracy computation\n",
    "- True/False Positive/Negative counting\n",
    "- Span match thresholding (default 0.70)\n",
    "\n",
    "CHANGES MADE (Colab Adaptations Only):\n",
    "=====================================\n",
    "1. Class names: Added \"Colab\" prefix to avoid conflicts\n",
    "2. Import statements: Duplicated in cells for independence\n",
    "3. Service initialization: Added print statements for visibility\n",
    "4. Error handling: Enhanced for Colab environment feedback\n",
    "\n",
    "CONCLUSION:\n",
    "===========\n",
    "âœ… The Colab services are FUNCTIONALLY IDENTICAL to API services\n",
    "âœ… All core algorithms, thresholds, and logic are preserved\n",
    "âœ… Only cosmetic changes for Colab compatibility\n",
    "âœ… Expected behavior: API and Colab should produce identical results\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… VERIFICATION COMPLETE\")\n",
    "print(\"ðŸ“‹ Colab services are structurally identical to API services\")\n",
    "print(\"ðŸ”„ Only class names and environment adaptations differ\")\n",
    "print(\"âš¡ Core algorithms and logic are 100% preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de46d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Service (Colab Version)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "from polymer_extractor.storage.bucket_manager import BucketManager\n",
    "from polymer_extractor.storage.database_manager import DatabaseManager\n",
    "from polymer_extractor.utils.logging import Logger\n",
    "\n",
    "# Initialize services\n",
    "logger = Logger()\n",
    "db = DatabaseManager()\n",
    "bucket = BucketManager()\n",
    "\n",
    "class ColabEvaluationService:\n",
    "    def __init__(self):\n",
    "        self.datasets_collection = \"datasets_metadata\"\n",
    "        self.extraction_collection = \"extraction_metadata\"\n",
    "        self.results_dir = Path(WORKSPACE_DIR) / \"exports\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def evaluate(self, tei_path: str, span_match_threshold: float = 0.70) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full evaluation pipeline.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tei_path : str\n",
    "            Path to processed TEI XML file.\n",
    "        span_match_threshold : float, optional\n",
    "            Fuzzy match threshold for entity text comparison (default=0.70).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Evaluation summary with metrics and file paths.\n",
    "        \"\"\"\n",
    "        base_name = Path(tei_path).stem\n",
    "        logger.info(f\"[Evaluation] Starting evaluation for {base_name}\",\n",
    "                    source=\"ColabEvaluationService.evaluate\")\n",
    "\n",
    "        # 1. Find matching dataset\n",
    "        dataset_entry = self._find_matching_dataset(base_name)\n",
    "        if not dataset_entry:\n",
    "            logger.error(f\"No matching test dataset found for {base_name}\",\n",
    "                         source=\"ColabEvaluationService.evaluate\")\n",
    "            return {\"success\": False, \"message\": \"No matching testing dataset found.\"}\n",
    "\n",
    "        # 2. Download and load dataset\n",
    "        dataset_path = self._download_dataset(dataset_entry)\n",
    "        groundtruth_df = pd.read_csv(dataset_path)\n",
    "\n",
    "        # 3. Load predictions\n",
    "        predictions = self._load_predictions(base_name)\n",
    "        if not predictions:\n",
    "            return {\"success\": False, \"message\": \"No predictions found for evaluation.\"}\n",
    "\n",
    "        # 4. Normalize both datasets\n",
    "        gt_entities = self._normalize_groundtruth(groundtruth_df)\n",
    "        pred_entities = self._normalize_predictions(predictions)\n",
    "\n",
    "        # 5. Compute metrics\n",
    "        metrics, detailed_df = self._compute_metrics(gt_entities, pred_entities, span_match_threshold)\n",
    "\n",
    "        # 6. Save evaluation results\n",
    "        csv_path = self.results_dir / f\"evaluation_results_{base_name}.csv\"\n",
    "        detailed_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        self._save_to_metadata(base_name, metrics, csv_path)\n",
    "\n",
    "        logger.info(f\"[Evaluation] Completed evaluation for {base_name}\",\n",
    "                    source=\"ColabEvaluationService.evaluate\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"file_evaluated\": base_name,\n",
    "            \"matched_dataset\": dataset_entry.get(\"original_filename\"),\n",
    "            \"metrics\": metrics,\n",
    "            \"summary\": {\n",
    "                \"total_groundtruth_entities\": len(gt_entities),\n",
    "                \"total_predicted_entities\": len(pred_entities)\n",
    "            },\n",
    "            \"results_csv\": str(csv_path),\n",
    "            \"saved_to_models_metadata\": True\n",
    "        }\n",
    "\n",
    "    def _find_matching_dataset(self, file_stem: str) -> Dict[str, Any]:\n",
    "        \"\"\"Find dataset entry in Appwrite matching the file name or fuzzy match.\"\"\"\n",
    "        datasets = db.list_documents(self.datasets_collection)\n",
    "        candidates = [d for d in datasets if d.get(\"type\") == \"testing\"]\n",
    "\n",
    "        for entry in candidates:\n",
    "            target_input = entry.get(\"target_input\", \"\")\n",
    "            if not target_input:\n",
    "                continue\n",
    "            if file_stem in target_input or target_input in file_stem:\n",
    "                return entry\n",
    "            # Handle underscore suffix case\n",
    "            if \"_\" in file_stem and file_stem.split(\"_\")[0] == Path(target_input).stem:\n",
    "                return entry\n",
    "        return None\n",
    "\n",
    "    def _download_dataset(self, entry: Dict[str, Any]) -> str:\n",
    "        \"\"\"Download dataset CSV locally.\"\"\"\n",
    "        file_url = entry.get(\"file_url\")\n",
    "        file_name = entry.get(\"original_filename\") or entry.get(\"dataset_name\") + \".csv\"\n",
    "        local_path = Path(WORKSPACE_DIR) / \"datasets\" / \"testing\" / file_name\n",
    "        os.makedirs(local_path.parent, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            bucket.download_file(\"datasets_bucket\", file_url, str(local_path))\n",
    "            return str(local_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download dataset: {e}\",\n",
    "                         source=\"ColabEvaluationService._download_dataset\", error=e)\n",
    "            raise\n",
    "\n",
    "    def _load_predictions(self, base_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load ensemble inference results (local JSON preferred, fallback Appwrite).\"\"\"\n",
    "        local_json = self.results_dir / f\"{base_name}_ensemble_results.json\"\n",
    "        if local_json.exists():\n",
    "            with open(local_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        try:\n",
    "            doc = db.get_document(self.extraction_collection, base_name)\n",
    "            if doc and doc.get(\"extracted_entities\"):\n",
    "                return json.loads(doc[\"extracted_entities\"])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load predictions: {e}\",\n",
    "                         source=\"ColabEvaluationService._load_predictions\", error=e)\n",
    "        return None\n",
    "\n",
    "    def _normalize_groundtruth(self, df: pd.DataFrame) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert ground-truth CSV to long-format entity list.\"\"\"\n",
    "        entities = []\n",
    "        entity_patterns = [\"polymer\", \"property\", \"value\", \"unit\", \"symbol\", \"material\"]\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            sentence = row.get(\"sentence\", \"\")\n",
    "            for pattern in entity_patterns:\n",
    "                cols = [c for c in df.columns if c.lower().startswith(pattern)]\n",
    "                for c in cols:\n",
    "                    value = str(row.get(c, \"\")).strip()\n",
    "                    if value and value != \"nan\" and len(value) > 1:\n",
    "                        entities.append({\n",
    "                            \"sentence\": sentence,\n",
    "                            \"entity_type\": pattern.upper(),\n",
    "                            \"entity_text\": value\n",
    "                        })\n",
    "        return entities\n",
    "\n",
    "    def _normalize_predictions(self, predictions: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert predictions JSON to long-format entity list.\"\"\"\n",
    "        entities = []\n",
    "        for ent_type, ents in predictions.items():\n",
    "            for ent in ents:\n",
    "                entities.append({\n",
    "                    \"sentence\": None,\n",
    "                    \"entity_type\": ent_type.upper(),\n",
    "                    \"entity_text\": ent[\"text\"].strip()\n",
    "                })\n",
    "        return entities\n",
    "\n",
    "    def _compute_metrics(\n",
    "            self,\n",
    "            groundtruth: List[Dict[str, str]],\n",
    "            predictions: List[Dict[str, str]],\n",
    "            threshold: float\n",
    "    ) -> Tuple[Dict[str, float], pd.DataFrame]:\n",
    "        \"\"\"Compute precision, recall, F1, accuracy with fuzzy span matching.\"\"\"\n",
    "        matches = []\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "\n",
    "        gt_used = set()\n",
    "        for pred in predictions:\n",
    "            pred_text = pred[\"entity_text\"].lower()\n",
    "            pred_type = pred[\"entity_type\"]\n",
    "\n",
    "            best_match = None\n",
    "            best_score = 0.0\n",
    "            for i, gt in enumerate(groundtruth):\n",
    "                if i in gt_used or gt[\"entity_type\"] != pred_type:\n",
    "                    continue\n",
    "                    \n",
    "                score = difflib.SequenceMatcher(None, pred_text, gt[\"entity_text\"].lower()).ratio()\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_match = i\n",
    "\n",
    "            if best_match is not None and best_score >= threshold:\n",
    "                tp += 1\n",
    "                gt_used.add(best_match)\n",
    "                matches.append({\n",
    "                    \"prediction\": pred[\"entity_text\"],\n",
    "                    \"groundtruth\": groundtruth[best_match][\"entity_text\"],\n",
    "                    \"entity_type\": pred_type,\n",
    "                    \"match_score\": best_score,\n",
    "                    \"status\": \"TP\"\n",
    "                })\n",
    "            else:\n",
    "                fp += 1\n",
    "                matches.append({\n",
    "                    \"prediction\": pred[\"entity_text\"],\n",
    "                    \"groundtruth\": \"\",\n",
    "                    \"entity_type\": pred_type,\n",
    "                    \"match_score\": best_score,\n",
    "                    \"status\": \"FP\"\n",
    "                })\n",
    "\n",
    "        # Add false negatives\n",
    "        for i, gt in enumerate(groundtruth):\n",
    "            if i not in gt_used:\n",
    "                fn += 1\n",
    "                matches.append({\n",
    "                    \"prediction\": \"\",\n",
    "                    \"groundtruth\": gt[\"entity_text\"],\n",
    "                    \"entity_type\": gt[\"entity_type\"],\n",
    "                    \"match_score\": 0.0,\n",
    "                    \"status\": \"FN\"\n",
    "                })\n",
    "\n",
    "        fn = len(groundtruth) - len(gt_used)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = tp / len(groundtruth) if len(groundtruth) > 0 else 0\n",
    "\n",
    "        metrics = {\n",
    "            \"precision\": round(precision, 4),\n",
    "            \"recall\": round(recall, 4),\n",
    "            \"f1_score\": round(f1, 4),\n",
    "            \"accuracy\": round(accuracy, 4),\n",
    "            \"true_positives\": tp,\n",
    "            \"false_positives\": fp,\n",
    "            \"false_negatives\": fn\n",
    "        }\n",
    "\n",
    "        return metrics, pd.DataFrame(matches)\n",
    "\n",
    "    def _save_to_metadata(self, base_name: str, metrics: Dict[str, Any], csv_path: Path):\n",
    "        \"\"\"Save evaluation metrics to models_metadata and upload CSV.\"\"\"\n",
    "        try:\n",
    "            bucket_id = \"model_results_bucket\"\n",
    "            bucket.create_bucket(bucket_id, \"Model evaluation results\")\n",
    "            uploaded = bucket.upload_file(bucket_id, str(csv_path))\n",
    "\n",
    "            db.create_document(\"models_metadata\", {\n",
    "                \"file_name\": base_name,\n",
    "                \"metrics\": json.dumps(metrics),\n",
    "                \"results_csv\": uploaded.get(\"$id\", \"\"),\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "            })\n",
    "\n",
    "            logger.info(f\"Saved evaluation results for {base_name}\",\n",
    "                        source=\"ColabEvaluationService._save_to_metadata\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save evaluation results: {e}\",\n",
    "                         source=\"ColabEvaluationService._save_to_metadata\", error=e)\n",
    "\n",
    "# Initialize the evaluation service\n",
    "evaluation_service = ColabEvaluationService()\n",
    "print(\"[INFO] Evaluation Service initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757921a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Ensemble Inference\n",
    "\n",
    "# Check if models are ready first\n",
    "if not models_ready:\n",
    "    print(\"[ERROR] Fine-tuned models are not available. Please run the fine-tuning cell first.\")\n",
    "    print(\"ðŸ’¡ Go to cell 7 (Fine-tuning pipeline) and run it to create the required models.\")\n",
    "else:\n",
    "    # Get the processed TEI path from previous pipeline\n",
    "    processed_tei_path = token_result.get('processed_path') or result['tei_result'].get('processed_path')\n",
    "\n",
    "    if processed_tei_path and os.path.exists(processed_tei_path):\n",
    "        print(f\"[INFO] Running ensemble inference on: {processed_tei_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Run ensemble inference\n",
    "            inference_result = ensemble_service.run_inference(processed_tei_path)\n",
    "            \n",
    "            print(f\"[SUCCESS] Ensemble inference completed!\")\n",
    "            print(f\"Models used: {inference_result['models_used']}\")\n",
    "            print(f\"Total entities extracted: {inference_result['num_entities']}\")\n",
    "            print(f\"Results saved to: {inference_result['output_file']}\")\n",
    "            \n",
    "            # Display sample results\n",
    "            if os.path.exists(inference_result['output_file']):\n",
    "                with open(inference_result['output_file'], 'r', encoding='utf-8') as f:\n",
    "                    results = json.load(f)\n",
    "                \n",
    "                print(\"\\n[SAMPLE RESULTS]\")\n",
    "                for entity_type, entities in results.items():\n",
    "                    if entities:\n",
    "                        print(f\"\\n{entity_type}:\")\n",
    "                        for i, ent in enumerate(entities[:3]):  # Show first 3 entities\n",
    "                            print(f\"  {i+1}. {ent['text']} (confidence: {ent['confidence']})\")\n",
    "                        if len(entities) > 3:\n",
    "                            print(f\"  ... and {len(entities) - 3} more\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Ensemble inference failed: {str(e)}\")\n",
    "            print(\"This might be due to missing model files or incompatible model formats.\")\n",
    "    else:\n",
    "        print(\"[ERROR] No valid processed TEI path found from previous pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation Service\n",
    "\n",
    "# Run evaluation if inference was successful\n",
    "if 'inference_result' in locals() and inference_result.get('success'):\n",
    "    print(f\"[INFO] Running evaluation on: {processed_tei_path}\")\n",
    "    \n",
    "    # Run evaluation with default threshold\n",
    "    eval_result = evaluation_service.evaluate(\n",
    "        tei_path=processed_tei_path,\n",
    "        span_match_threshold=0.70\n",
    "    )\n",
    "    \n",
    "    if eval_result.get('success'):\n",
    "        print(f\"[SUCCESS] Evaluation completed!\")\n",
    "        print(f\"File evaluated: {eval_result['file_evaluated']}\")\n",
    "        print(f\"Matched dataset: {eval_result['matched_dataset']}\")\n",
    "        \n",
    "        # Display metrics\n",
    "        metrics = eval_result['metrics']\n",
    "        print(f\"\\n[EVALUATION METRICS]\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"True Positives: {metrics['true_positives']}\")\n",
    "        print(f\"False Positives: {metrics['false_positives']}\")\n",
    "        print(f\"False Negatives: {metrics['false_negatives']}\")\n",
    "        \n",
    "        # Display summary\n",
    "        summary = eval_result['summary']\n",
    "        print(f\"\\n[SUMMARY]\")\n",
    "        print(f\"Total ground truth entities: {summary['total_groundtruth_entities']}\")\n",
    "        print(f\"Total predicted entities: {summary['total_predicted_entities']}\")\n",
    "        print(f\"Results CSV saved to: {eval_result['results_csv']}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"[WARNING] Evaluation failed: {eval_result.get('message', 'Unknown error')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"[ERROR] No inference result available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fine-tuning pipeline for Polymer NLP Extractor\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==== 1. LOAD CONFIGURATION ====\n",
    "load_dotenv()\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "from polymer_extractor.model_config import ENSEMBLE_MODELS, LABELS, LABEL2ID, ID2LABEL\n",
    "from polymer_extractor.utils.paths import WORKSPACE_DIR\n",
    "from polymer_extractor.services.constants.templates import SENTENCE_TEMPLATES\n",
    "from polymer_extractor.services.constants.polymer_names import POLYMER_NAMES\n",
    "from polymer_extractor.services.constants.property_names import PROPERTY_NAMES\n",
    "from polymer_extractor.services.constants.scientific_units import SCIENTIFIC_UNITS\n",
    "from polymer_extractor.services.constants.scientific_symbols import SCIENTIFIC_SYMBOLS\n",
    "from polymer_extractor.services.constants.material_names import MATERIAL_NAMES\n",
    "from polymer_extractor.services.constants.value_formats import VALUE_FORMATS\n",
    "\n",
    "TRAINING_DIR = Path(WORKSPACE_DIR) / \"datasets\" / \"training\"\n",
    "TESTING_DIR = Path(WORKSPACE_DIR) / \"datasets\" / \"testing\"\n",
    "OUTPUT_DIR = Path(WORKSPACE_DIR) / \"models\" / \"finetuned\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==== 2. DATASET LOADING ====\n",
    "\n",
    "def get_real_datasets(override: bool = False, hours: int = 48) -> List[pd.DataFrame]:\n",
    "    now = datetime.now()\n",
    "    real_data = []\n",
    "    for folder in [TRAINING_DIR, TESTING_DIR]:\n",
    "        for file in folder.glob(\"*.csv\"):\n",
    "            mtime = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "            if override or (now - mtime) < timedelta(hours=hours):\n",
    "                try:\n",
    "                    df = pd.read_csv(file).dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "                    real_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Skipped {file.name}: {e}\")\n",
    "    return real_data\n",
    "\n",
    "def generate_synthetic_data(n: int = 25000) -> List[Dict[str, str]]:\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        p, pr, u, s, v, m = map(random.choice, [\n",
    "            POLYMER_NAMES, PROPERTY_NAMES, SCIENTIFIC_UNITS,\n",
    "            SCIENTIFIC_SYMBOLS, VALUE_FORMATS, MATERIAL_NAMES\n",
    "        ])\n",
    "        sentence = random.choice(SENTENCE_TEMPLATES).format(\n",
    "            polymer=p, property=pr, unit=u, symbol=s, value=v, material=m\n",
    "        )\n",
    "        data.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"polymer\": p, \"property\": pr, \"unit\": u,\n",
    "            \"symbol\": s, \"value\": v, \"material\": m\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# ==== 3. TOKENIZATION + LABELING ====\n",
    "\n",
    "def tokenize_and_label(sample, tokenizer):\n",
    "    sent = sample[\"sentence\"]\n",
    "    entities = {\n",
    "        \"POLYMER\": sample.get(\"polymer\", \"\"),\n",
    "        \"PROPERTY\": sample.get(\"property\", \"\"),\n",
    "        \"UNIT\": sample.get(\"unit\", \"\"),\n",
    "        \"SYMBOL\": sample.get(\"symbol\", \"\"),\n",
    "        \"VALUE\": sample.get(\"value\", \"\"),\n",
    "        \"MATERIAL\": sample.get(\"material\", \"\")\n",
    "    }\n",
    "\n",
    "    encoding = tokenizer(sent, return_offsets_mapping=True, truncation=True)\n",
    "    tokens = encoding.tokens()\n",
    "    offsets = encoding.offset_mapping\n",
    "    label_map = [\"O\"] * len(sent)\n",
    "\n",
    "    for ent, val in entities.items():\n",
    "        if not val or len(val) < 2:\n",
    "            continue\n",
    "        for match in re.finditer(re.escape(val), sent):\n",
    "            for i in range(match.start(), match.end()):\n",
    "                label_map[i] = f\"I-{ent}\"\n",
    "            label_map[match.start()] = f\"B-{ent}\"\n",
    "\n",
    "    labels = []\n",
    "    for start, end in offsets:\n",
    "        if start == end:\n",
    "            labels.append(\"O\")\n",
    "        else:\n",
    "            span = label_map[start:end]\n",
    "            label = next((l for l in span if l != \"O\"), \"O\")\n",
    "            labels.append(label)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoding.input_ids,\n",
    "        \"attention_mask\": encoding.attention_mask,\n",
    "        \"labels\": [LABEL2ID.get(lbl, 0) for lbl in labels]\n",
    "    }\n",
    "\n",
    "def to_dataset(encodings: List[Dict[str, Any]]) -> Dataset:\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": [e[\"input_ids\"] for e in encodings],\n",
    "        \"attention_mask\": [e[\"attention_mask\"] for e in encodings],\n",
    "        \"labels\": [e[\"labels\"] for e in encodings]\n",
    "    })\n",
    "\n",
    "# ==== 4. DATASET PREPARATION ====\n",
    "\n",
    "override_real = False  # â† change to True to force all real data to be used\n",
    "real_dfs = get_real_datasets(override=override_real)\n",
    "real_samples = [{\"sentence\": row[\"sentence\"]} for df in real_dfs for _, row in df.iterrows()]\n",
    "\n",
    "print(f\"[INFO] Found {len(real_samples)} real samples.\")\n",
    "\n",
    "# Create final training corpus\n",
    "synthetic = generate_synthetic_data(n=25000)\n",
    "corpus = synthetic + real_samples\n",
    "random.shuffle(corpus)\n",
    "\n",
    "split = int(len(corpus) * 0.9)\n",
    "train_samples, val_samples = corpus[:split], corpus[split:]\n",
    "\n",
    "print(f\"[INFO] Training with {len(train_samples)} | Validation with {len(val_samples)}\")\n",
    "\n",
    "# ==== 5. TRAIN EACH MODEL ====\n",
    "\n",
    "for model_cfg in ENSEMBLE_MODELS:\n",
    "    print(f\"\\n[TRAINING] {model_cfg.name} ({model_cfg.model_id})\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_cfg.model_id, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_cfg.model_id,\n",
    "        num_labels=len(LABELS),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "        ignore_mismatched_sizes=True  # Add this line to ignore the size mismatch\n",
    "    )\n",
    "\n",
    "    train_encoded = [tokenize_and_label(s, tokenizer) for s in train_samples]\n",
    "    val_encoded = [tokenize_and_label(s, tokenizer) for s in val_samples]\n",
    "    train_ds = to_dataset(train_encoded)\n",
    "    val_ds = to_dataset(val_encoded)\n",
    "\n",
    "    out_path = OUTPUT_DIR / model_cfg.name\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(out_path),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=model_cfg.training_config.get(\"lr\", 2e-5),\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=model_cfg.training_config.get(\"epochs\", 5),\n",
    "        weight_decay=model_cfg.training_config.get(\"weight_decay\", 0.01),\n",
    "        gradient_accumulation_steps=4,\n",
    "        logging_dir=str(out_path / \"logs\"),\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[\"wandb\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(out_path))\n",
    "    print(f\"[SAVED] {model_cfg.name} â†’ {out_path}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del trainer, model, tokenizer, train_ds, val_ds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the finetuned models directory\n",
    "!zip -r {PROJECT_ROOT}/finetuned_models.zip {PROJECT_ROOT}/workspace/models/finetuned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
