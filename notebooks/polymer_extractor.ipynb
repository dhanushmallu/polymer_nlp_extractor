{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a408111",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import grobid\n",
    "!wget https://github.com/kermitt2/grobid/archive/0.8.2.zip\n",
    "!unzip 0.8.2.zip -d /content/polymer_nlp_extractor/workspace\n",
    "!rm 0.8.2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e70e6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# add root directory of the project to python path\n",
    "import sys\n",
    "sys.path.append('/content/polymer_nlp_extractor')\n",
    "PROJECT_ROOT = '/content/polymer_nlp_extractor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2e7e7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install dependencies with logs\n",
    "! pip install -r {PROJECT_ROOT}/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset variable\n",
    "RESET_BACKEND = False\n",
    "\n",
    "from polymer_extractor.services.setup_service import SetupService\n",
    "from fastapi import HTTPException\n",
    "\n",
    "setup_service = SetupService()\n",
    "\n",
    "if RESET_BACKEND:\n",
    "    try:\n",
    "        # Get database and bucket managers from setup service\n",
    "        db_manager = setup_service.get_database_manager()\n",
    "        bucket_manager = setup_service.get_bucket_manager()\n",
    "        \n",
    "        # Delete collections\n",
    "        collections = db_manager.list_collections()\n",
    "        for col in collections:\n",
    "            if col['$id'] != \"system_logs\":  # Skip Logger's autonomous collection\n",
    "                db_manager.delete_collection(col['$id'])\n",
    "\n",
    "        # Delete buckets\n",
    "        buckets = bucket_manager.list_buckets()\n",
    "        for bkt in buckets:\n",
    "            bucket_manager.delete_bucket(bkt['$id'])\n",
    "\n",
    "        # Reinitialize\n",
    "        setup_service.initialize_all_resources()\n",
    "        print(\"System reset and reinitialized successfully.\")\n",
    "        result = {\"status\": \"success\", \"message\": \"System reset and reinitialized successfully.\"}\n",
    "        print(f\"Reset completed: {result['message']}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to reset system: {str(e)}\"\n",
    "        print(f\"ERROR: Failed to reset system: {str(e)}\")\n",
    "        print(f\"Reset failed: {error_msg}\")\n",
    "        raise HTTPException(status_code=500, detail=error_msg)\n",
    "else:\n",
    "    try:\n",
    "        setup_service.initialize_all_resources()\n",
    "        print(\"System initialized successfully.\")\n",
    "        print(\"System initialization completed successfully.\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to initialize system: {str(e)}\"\n",
    "        print(f\"ERROR: Failed to initialize system: {str(e)}\")\n",
    "        print(f\"Initialization failed: {error_msg}\")\n",
    "        raise HTTPException(status_code=500, detail=error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from polymer_extractor.services import grobid_service\n",
    "from polymer_extractor.services.grobid_service import GrobidService\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# === Response Models ===\n",
    "\n",
    "class ServerStatusResponse(BaseModel):\n",
    "    \"\"\"Response model for server status checks.\"\"\"\n",
    "    status: str\n",
    "    message: str\n",
    "    server_url: str\n",
    "\n",
    "# Define workspace directory if not already defined\n",
    "WORKSPACE_DIR = os.path.join(PROJECT_ROOT, \"workspace\")\n",
    "\n",
    "# Initialize GrobidService\n",
    "grobid_service = GrobidService()\n",
    "\n",
    "# Check if Grobid is running\n",
    "try:\n",
    "    grobid_service.check_server_status()\n",
    "    grobid_is_running = True\n",
    "    grobid_status_response = ServerStatusResponse(\n",
    "        status=\"running\",\n",
    "        message=\"GROBID server is alive and responding\",\n",
    "        server_url=grobid_service.grobid_server_url\n",
    "    )\n",
    "    print(\"GROBID server is alive and responding\")\n",
    "except Exception as e:\n",
    "    grobid_is_running = False\n",
    "    print(f\"WARNING: GROBID server status check failed: {e}\")\n",
    "    grobid_status_response = ServerStatusResponse(\n",
    "        status=\"unreachable\",\n",
    "        message=f\"GROBID server is not responding: {str(e)}\",\n",
    "        server_url=grobid_service.grobid_server_url\n",
    "    )\n",
    "\n",
    "# If Grobid is not running, start the server\n",
    "if not grobid_is_running:\n",
    "    try:\n",
    "        grobid_home = os.path.join(WORKSPACE_DIR, \"grobid-0.8.2\")\n",
    "        print(\"Starting GROBID server via API request\")\n",
    "        grobid_service.start_server(grobid_home=grobid_home)\n",
    "        grobid_status_response = ServerStatusResponse(\n",
    "            status=\"started\",\n",
    "            message=\"GROBID server started successfully\",\n",
    "            server_url=grobid_service.grobid_server_url\n",
    "        )\n",
    "        print(\"GROBID server started successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to start GROBID server via API: {e}\")\n",
    "        raise Exception(f\"Failed to start GROBID server: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a File\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from fastapi import HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "\n",
    "# Request models\n",
    "class TokenPackRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Request body for /api/preprocess/tokenpack\n",
    "    \"\"\"\n",
    "    tei_path: str = Field(\n",
    "        ...,\n",
    "        description=\"Absolute filesystem path to cleaned TEI XML file.\"\n",
    "    )\n",
    "\n",
    "class TEIProcessRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Request body for /api/preprocess/tei\n",
    "    \"\"\"\n",
    "    tei_path: str = Field(\n",
    "        ...,\n",
    "        description=\"Absolute filesystem path to the TEI XML file for cleaning and metadata extraction.\"\n",
    "    )\n",
    "\n",
    "class ProcessingResult(BaseModel):\n",
    "    success: bool\n",
    "    message: str\n",
    "    original_file: str\n",
    "    pdf_file: str = None\n",
    "    metadata: dict = {}\n",
    "    local_tei_path: str = None\n",
    "    storage_success: bool = False\n",
    "    storage_errors: list = []\n",
    "\n",
    "# Mock services and functions - replace with actual implementations\n",
    "class MockFile:\n",
    "    def __init__(self, filepath):\n",
    "        self.filename = os.path.basename(filepath)\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    async def read(self):\n",
    "        with open(self.filepath, 'rb') as f:\n",
    "            return f.read()\n",
    "\n",
    "class MockGrobidService:\n",
    "    def process_document(self, temp_path, filename_stem, original_filename):\n",
    "        return {\n",
    "            'pdf_file': str(temp_path),\n",
    "            'metadata': {'title': 'Test Document'},\n",
    "            'local_tei_path': str(temp_path).replace('.pdf', '.xml'),\n",
    "            'storage_success': True,\n",
    "            'storage_errors': []\n",
    "        }\n",
    "\n",
    "class MockTEIProcessingService:\n",
    "    def process(self, tei_path):\n",
    "        return {\"success\": True, \"processed_path\": tei_path.replace('.xml', '_processed.xml')}\n",
    "\n",
    "class MockTokenizerService:\n",
    "    def audit_and_extend_all(self, force=False):\n",
    "        return {\"tokenizers_updated\": 3, \"force_rebuild\": force}\n",
    "\n",
    "class MockTokenPackingService:\n",
    "    def process(self, tei_path):\n",
    "        return {\"success\": True, \"packed_path\": tei_path.replace('.xml', '_packed.json')}\n",
    "\n",
    "def cleanup_temp_file(temp_path):\n",
    "    if temp_path and os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "\n",
    "# Initialize services and variables\n",
    "WORKSPACE_DIR = os.path.join(WORKSPACE_DIR, \"workspace\")\n",
    "grobid_service = MockGrobidService()\n",
    "\n",
    "async def process_file_pipeline():\n",
    "    \"\"\"Main processing pipeline for a file\"\"\"\n",
    "    \n",
    "    file_path = os.path.join(WORKSPACE_DIR, \"raw_inputs\", \"057.pdf\")  # Example file path\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise HTTPException(status_code=404, detail=f\"File not found: {file_path}\")\n",
    "    \n",
    "    file = MockFile(file_path)\n",
    "    \n",
    "    # Step 1: Upload a file via grobid and retain its path for step 2\n",
    "    temp_path = None\n",
    "    try:\n",
    "        allowed_extensions = {'.pdf', '.xml', '.html', '.htm'}\n",
    "        file_ext = Path(file.filename).suffix.lower()\n",
    "        if file_ext not in allowed_extensions:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}\"\n",
    "            )\n",
    "\n",
    "        original_stem = Path(file.filename).stem\n",
    "        temp_dir = Path(tempfile.gettempdir())\n",
    "        temp_filename = f\"{original_stem}_{int(time.time())}{file_ext}\"\n",
    "        temp_path = temp_dir / temp_filename\n",
    "\n",
    "        with open(temp_path, 'wb') as temp_file:\n",
    "            content = await file.read()\n",
    "            temp_file.write(content)\n",
    "\n",
    "        result = grobid_service.process_document(\n",
    "            temp_path,\n",
    "            filename_stem=original_stem,\n",
    "            original_filename=file.filename\n",
    "        )\n",
    "\n",
    "        grobid_result = ProcessingResult(\n",
    "            success=True,\n",
    "            message=f\"Successfully processed {file.filename}\",\n",
    "            original_file=file.filename,\n",
    "            pdf_file=result.get('pdf_file'),\n",
    "            metadata=result.get('metadata', {}),\n",
    "            local_tei_path=result.get('local_tei_path'),\n",
    "            storage_success=result.get('storage_success', False),\n",
    "            storage_errors=result.get('storage_errors', [])\n",
    "        )\n",
    "        \n",
    "        tei_path = result.get('local_tei_path')\n",
    "\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Processing failed for uploaded file - {e}\")\n",
    "        if temp_path and temp_path.exists():\n",
    "            cleanup_temp_file(temp_path)\n",
    "        raise HTTPException(status_code=500, detail=f\"Processing failed: {str(e)}\")\n",
    "\n",
    "    # Step 2: Take the grobid output path as input and perform processing of the xml\n",
    "    if not tei_path:\n",
    "        raise HTTPException(status_code=500, detail=\"No TEI path from grobid processing\")\n",
    "    \n",
    "    req_tei = TEIProcessRequest(tei_path=tei_path)\n",
    "    \n",
    "    print(f\"INFO: Received TEI processing request: tei_path={req_tei.tei_path}\")\n",
    "\n",
    "    if not os.path.isabs(req_tei.tei_path) or not os.path.exists(req_tei.tei_path):\n",
    "        print(f\"ERROR: TEI file not found: {req_tei.tei_path}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"TEI file not found: {req_tei.tei_path}\")\n",
    "\n",
    "    try:\n",
    "        service = MockTEIProcessingService()\n",
    "        tei_result = service.process(req_tei.tei_path)\n",
    "        processed_tei_path = tei_result.get('processed_path', req_tei.tei_path)\n",
    "        \n",
    "        print(f\"INFO: TEI processing completed for {req_tei.tei_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: TEI processing failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"TEI processing failed: {str(e)}\")\n",
    "\n",
    "    # Step 3: Attempt tokenization \n",
    "    force = False  # Define force parameter\n",
    "    print(f\"INFO: Received tokenizer audit request with force={force}\")\n",
    "    try:\n",
    "        tokenizer_service = MockTokenizerService()\n",
    "        audit_results = tokenizer_service.audit_and_extend_all(force=force)\n",
    "\n",
    "        print(\"INFO: Tokenizer audit completed successfully.\")\n",
    "        \n",
    "        tokenizer_result = {\n",
    "            \"success\": True,\n",
    "            \"message\": \"All tokenizers audited and extended successfully.\",\n",
    "            \"force_rebuild\": force,\n",
    "            \"audit_results\": audit_results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Tokenizer audit failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Tokenizer audit failed: {str(e)}\")\n",
    "\n",
    "    # Step 4: Take the output path of the xml processing in step 2 and use it for token packing\n",
    "    req_token = TokenPackRequest(tei_path=processed_tei_path)\n",
    "    \n",
    "    print(f\"INFO: Received token packing request: tei_path='{req_token.tei_path}'\")\n",
    "\n",
    "    if not os.path.isfile(req_token.tei_path):\n",
    "        print(f\"ERROR: TEI file does not exist: {req_token.tei_path}\")\n",
    "        raise HTTPException(status_code=404, detail=f\"TEI file not found: {req_token.tei_path}\")\n",
    "\n",
    "    try:\n",
    "        service = MockTokenPackingService()\n",
    "        token_result = service.process(tei_path=req_token.tei_path)\n",
    "\n",
    "        print(f\"INFO: Token packing completed for all models on {req_token.tei_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Token packing failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Token packing failed: {str(e)}\")\n",
    "\n",
    "    # Clean up temp file\n",
    "    if temp_path and temp_path.exists():\n",
    "        cleanup_temp_file(temp_path)\n",
    "\n",
    "    # Return combined results\n",
    "    return {\n",
    "        \"grobid_result\": grobid_result,\n",
    "        \"tei_result\": tei_result,\n",
    "        \"tokenizer_result\": tokenizer_result,\n",
    "        \"token_result\": token_result\n",
    "    }\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # For Jupyter notebook, you can run this:\n",
    "    result = asyncio.run(process_file_pipeline())\n",
    "    print(\"Pipeline defined. Run 'asyncio.run(process_file_pipeline())' to execute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024dbb0daff69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full fine-tuning pipeline for Polymer NLP Extractor\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==== 1. LOAD CONFIGURATION ====\n",
    "load_dotenv()\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "from polymer_extractor.model_config import ENSEMBLE_MODELS, LABELS, LABEL2ID, ID2LABEL\n",
    "from polymer_extractor.utils.paths import WORKSPACE_DIR\n",
    "from polymer_extractor.services.constants.templates import SENTENCE_TEMPLATES\n",
    "from polymer_extractor.services.constants.polymer_names import POLYMER_NAMES\n",
    "from polymer_extractor.services.constants.property_names import PROPERTY_NAMES\n",
    "from polymer_extractor.services.constants.scientific_units import SCIENTIFIC_UNITS\n",
    "from polymer_extractor.services.constants.scientific_symbols import SCIENTIFIC_SYMBOLS\n",
    "from polymer_extractor.services.constants.material_names import MATERIAL_NAMES\n",
    "from polymer_extractor.services.constants.value_formats import VALUE_FORMATS\n",
    "\n",
    "TRAINING_DIR = Path(WORKSPACE_DIR) / \"datasets\" / \"training\"\n",
    "TESTING_DIR = Path(WORKSPACE_DIR) / \"datasets\" / \"testing\"\n",
    "OUTPUT_DIR = Path(WORKSPACE_DIR) / \"models\" / \"finetuned\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ==== 2. DATASET LOADING ====\n",
    "\n",
    "def get_real_datasets(override: bool = False, hours: int = 48) -> List[pd.DataFrame]:\n",
    "    now = datetime.now()\n",
    "    real_data = []\n",
    "    for folder in [TRAINING_DIR, TESTING_DIR]:\n",
    "        for file in folder.glob(\"*.csv\"):\n",
    "            mtime = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "            if override or (now - mtime) < timedelta(hours=hours):\n",
    "                try:\n",
    "                    df = pd.read_csv(file).dropna(subset=[\"sentence\"]).reset_index(drop=True)\n",
    "                    real_data.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Skipped {file.name}: {e}\")\n",
    "    return real_data\n",
    "\n",
    "def generate_synthetic_data(n: int = 25000) -> List[Dict[str, str]]:\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        p, pr, u, s, v, m = map(random.choice, [\n",
    "            POLYMER_NAMES, PROPERTY_NAMES, SCIENTIFIC_UNITS,\n",
    "            SCIENTIFIC_SYMBOLS, VALUE_FORMATS, MATERIAL_NAMES\n",
    "        ])\n",
    "        sentence = random.choice(SENTENCE_TEMPLATES).format(\n",
    "            polymer=p, property=pr, unit=u, symbol=s, value=v, material=m\n",
    "        )\n",
    "        data.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"polymer\": p, \"property\": pr, \"unit\": u,\n",
    "            \"symbol\": s, \"value\": v, \"material\": m\n",
    "        })\n",
    "    return data\n",
    "\n",
    "# ==== 3. TOKENIZATION + LABELING ====\n",
    "\n",
    "def tokenize_and_label(sample, tokenizer):\n",
    "    sent = sample[\"sentence\"]\n",
    "    entities = {\n",
    "        \"POLYMER\": sample.get(\"polymer\", \"\"),\n",
    "        \"PROPERTY\": sample.get(\"property\", \"\"),\n",
    "        \"UNIT\": sample.get(\"unit\", \"\"),\n",
    "        \"SYMBOL\": sample.get(\"symbol\", \"\"),\n",
    "        \"VALUE\": sample.get(\"value\", \"\"),\n",
    "        \"MATERIAL\": sample.get(\"material\", \"\")\n",
    "    }\n",
    "\n",
    "    encoding = tokenizer(sent, return_offsets_mapping=True, truncation=True)\n",
    "    tokens = encoding.tokens()\n",
    "    offsets = encoding.offset_mapping\n",
    "    label_map = [\"O\"] * len(sent)\n",
    "\n",
    "    for ent, val in entities.items():\n",
    "        if not val or len(val) < 2:\n",
    "            continue\n",
    "        for match in re.finditer(re.escape(val), sent):\n",
    "            for i in range(match.start(), match.end()):\n",
    "                label_map[i] = f\"I-{ent}\"\n",
    "            label_map[match.start()] = f\"B-{ent}\"\n",
    "\n",
    "    labels = []\n",
    "    for start, end in offsets:\n",
    "        if start == end:\n",
    "            labels.append(\"O\")\n",
    "        else:\n",
    "            span = label_map[start:end]\n",
    "            label = next((l for l in span if l != \"O\"), \"O\")\n",
    "            labels.append(label)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoding.input_ids,\n",
    "        \"attention_mask\": encoding.attention_mask,\n",
    "        \"labels\": [LABEL2ID.get(lbl, 0) for lbl in labels]\n",
    "    }\n",
    "\n",
    "def to_dataset(encodings: List[Dict[str, Any]]) -> Dataset:\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": [e[\"input_ids\"] for e in encodings],\n",
    "        \"attention_mask\": [e[\"attention_mask\"] for e in encodings],\n",
    "        \"labels\": [e[\"labels\"] for e in encodings]\n",
    "    })\n",
    "\n",
    "# ==== 4. DATASET PREPARATION ====\n",
    "\n",
    "override_real = False  # ← change to True to force all real data to be used\n",
    "real_dfs = get_real_datasets(override=override_real)\n",
    "real_samples = [{\"sentence\": row[\"sentence\"]} for df in real_dfs for _, row in df.iterrows()]\n",
    "\n",
    "print(f\"[INFO] Found {len(real_samples)} real samples.\")\n",
    "\n",
    "# Create final training corpus\n",
    "synthetic = generate_synthetic_data(n=25000)\n",
    "corpus = synthetic + real_samples\n",
    "random.shuffle(corpus)\n",
    "\n",
    "split = int(len(corpus) * 0.9)\n",
    "train_samples, val_samples = corpus[:split], corpus[split:]\n",
    "\n",
    "print(f\"[INFO] Training with {len(train_samples)} | Validation with {len(val_samples)}\")\n",
    "\n",
    "# ==== 5. TRAIN EACH MODEL ====\n",
    "\n",
    "for model_cfg in ENSEMBLE_MODELS:\n",
    "    print(f\"\\n[TRAINING] {model_cfg.name} ({model_cfg.model_id})\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_cfg.model_id, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_cfg.model_id,\n",
    "        num_labels=len(LABELS),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID\n",
    "    )\n",
    "\n",
    "    train_encoded = [tokenize_and_label(s, tokenizer) for s in train_samples]\n",
    "    val_encoded = [tokenize_and_label(s, tokenizer) for s in val_samples]\n",
    "    train_ds = to_dataset(train_encoded)\n",
    "    val_ds = to_dataset(val_encoded)\n",
    "\n",
    "    out_path = OUTPUT_DIR / model_cfg.name\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(out_path),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=model_cfg.training_config.get(\"lr\", 2e-5),\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=model_cfg.training_config.get(\"epochs\", 5),\n",
    "        weight_decay=model_cfg.training_config.get(\"weight_decay\", 0.01),\n",
    "        gradient_accumulation_steps=4,\n",
    "        logging_dir=str(out_path / \"logs\"),\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[\"wandb\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(str(out_path))\n",
    "    print(f\"[SAVED] {model_cfg.name} → {out_path}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del trainer, model, tokenizer, train_ds, val_ds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
