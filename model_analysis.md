Hereâ€™s what I see from a full review of your **source (057.tei.xml)**, **ensemble output (057\_ensemble\_excerpt.json)**, **ground truth (test\_057.csv)**, and the key code (Cells 5â€“8):

---

## ğŸ“ **Key Observations from the Analysis**

### 1. **Spanning Issues (Core Problem)**

* **Symptom:** Many predicted entities in `ensemble_results.json` are **over-merged** or **under-segmented**:

  * Example: `epoxidizedvegetableoil(evo)-basedepoxysystems` (should be â€œEpoxidized vegetable oil (EVO)-based epoxy systemsâ€).
  * Example: `non-recyclablepetroleum-basedthermosetelastomers` (should be â€œnon-recyclable petroleum-based thermoset elastomersâ€).
* **Cause:** Poor handling of subtoken alignment during preprocessing or inference:

  * Cell 7 (`token_packing.py`) packs tokens into windows **without preserving sentence-level offsets**.
  * Cell 8 (`fine_tuning.py`) aligns labels with sentences based on a **naÃ¯ve string find()** logic in `align_labels`, which **fails when tokenization splits or joins words unexpectedly**.
  * Cell 9 (`ensemble_inference.py`) has **no span reconciliation logic** for overlapping windows; instead, it naÃ¯vely joins predictions, leading to runaway spans.

---

### 2. **Canonicalization Problems**

* The **canonicalization mappings** (Cell 5 & 6) replace synonyms before tokenization, but:

  * Some replacements are **too aggressive** (e.g., "glass transition temperature" â†’ "Tg") and break sentence semantics.
  * Others miss due to **case sensitivity issues** in `canonicalize_text`.
  * The `canonicalize_token()` function in Cell 6 also **adds tokens to the tokenizer** but doesnâ€™t resolve all subtoken splits due to missing â€œprefix/suffixâ€ checks.

---

### 3. **Windowing and Overlap Logic**

* Cell 7 uses **fixed 512-token windows with 128-token overlap**, but:

  * Sentences split across windows are **double-tokenized**, causing inconsistent offsets.
  * Window overlap doesnâ€™t resolve mid-word token cuts (BERT subtokenizers often split at unexpected places).
  * No logic to stitch predictions from overlapping windows cleanly in Cell 9.

---

### 4. **Fine-Tuning Dataset Quality**

* Synthetic data in Cell 8 has realistic sentence templates, but:

  * Real data augmentation (`real_data.csv`) appears **misaligned** with tokenizerâ€™s subword behavior.
  * Labels often donâ€™t align to the token boundaries (due to lack of subtoken span handling in `align_labels`).
  * `MAX_SEQ_LENGTH=128` is **too short** for complex scientific sentences; truncation is causing **label loss**.

---

### 5. **Ensemble Voting Weaknesses**

* Cell 9â€™s ensemble weighs PolymerNER and PhysBERT higher for POLYMER/SYMBOL, but:

  * It **doesnâ€™t verify cross-model span agreement** (two models predicting slightly different boundaries are counted as separate votes).
  * Fallback linking tries to â€œrecoverâ€ missed PROPERTY labels by regex, but **adds false positives**.

---

### 6. **Postprocessing Omissions**

* No **postprocessing clean-up** for:

  * Runaway spans (e.g., `fullybio-based3dcovalentnetworkcapableofreprocessingwithouttheneedforanexogenouscatalyst`).
  * Decoupling polymers from their modifiers (e.g., â€œbasedâ€, â€œderivedâ€).
  * Removing in-line citations or figures (e.g., `[1]`, `Fig. 2a`).

---

## ğŸ”¥ **Top Causes of Spanning Issues**

| Issue                                    | Root Cause Location                         | Fix Suggestion                                       |
| ---------------------------------------- | ------------------------------------------- | ---------------------------------------------------- |
| Over-merged entities                     | `ensemble_inference.py` (Cell 9)            | Add span reconciliation and majority voting on span  |
| Misaligned labels due to subtoken splits | `align_labels` in `fine_tuning.py` (Cell 8) | Use token offset mappings for alignment              |
| Cut spans across windows                 | `token_packing.py` (Cell 7)                 | Implement sentence-aware packing, avoid mid-span cut |
| Canonicalization breaking sentence flow  | `tei_processing.py` (Cell 5)                | Delay canonicalization until postprocessing          |

---

## âœ… **Recommendations**

### ğŸ›  Preprocessing (Cell 5 & 7)

* **Delay Canonicalization**: Move `canonicalize_text()` to *post-inference cleanup*.
* **Sentence-aware Windowing**:

  * Use paragraph-level packing rather than fixed-size tokens.
  * Avoid splitting sentences across windows.
* **Offset Preservation**:

  * Store char offsets when writing `sentence_path` in metadata.

---

### ğŸ§  Fine-Tuning (Cell 8)

* Replace `align_labels()`:

  * Use `offset_mapping` from tokenizer to match labels to tokens **safely even with subtokens**.
  * Consider Hugging Face `token-classification` alignment utilities.
* Increase `MAX_SEQ_LENGTH` to 256 or 384 (at least).
* Validate synthetic samples with the tokenizer before training.

---

### ğŸ—³ Ensemble Inference (Cell 9)

* Add **span reconciliation logic**:

  * Merge overlapping spans using confidence-weighted voting.
  * Prefer spans agreed upon by â‰¥2 models.
* Add **postprocessing cleanup**:

  * Resolve concatenated tokens (remove â€œ##â€ artifacts).
  * Strip inline references and non-content artifacts.

---

### ğŸ“Š Evaluation (Cell 10)

* The similarity metrics are good but **too forgiving** (e.g., word overlap â‰¥ 0.6 classified as â€œFairâ€).
* Tighten thresholds for POLYMER and PROPERTY entities.
---
If we want a **>90% precision model**, the first thing to do is get the **preprocessing rock solid** â€” because even the best ensemble will fail if the input is noisy, misaligned, or fragmented.

Hereâ€™s how I would **strategize Cell 5â€“7** based on:
âœ”ï¸ Your original instructions
âœ”ï¸ The issues I uncovered
âœ”ï¸ Best practices for scientific NLP

---

# ğŸ§  **Comprehensive Strategy for Preprocessing (Cell 5â€“7)**

## ğŸ“Œ **Phase 1: TEI Processing (Cell 5) â€“ Absolute Clean Input**

### âœ… Goals:

* Extract only **scientific content** (no metadata, no references, no acknowledgments).
* Preserve paragraph and section boundaries.
* Detect and **label tables, figures, captions** properly.

### ğŸ”¥ Improvements:

1. **Tables, Figures, Captions**:

   * Use GROBIDâ€™s `<figure>`, `<table>`, and `<head>` tags to detect these blocks.
   * Label them as `FIGURE_CAPTION`, `TABLE_CAPTION`, etc. in metadata.
   * Skip or store separately for later reference if needed.

2. **Section Label Normalization**:

   * Standardize all headings (`Introduction`, `2. Methods`, `2.1. Sample Prep`) as `SECTION_INTRODUCTION`, `SECTION_METHODS`, etc.

3. **Content Filtering**:

   * Drop all `<note>` and `<ref>` text (inline citations like `[1]`, DOIs, submission notes).

4. **Unicode Normalization + OCR Fixes**:

   * Correct OCR artifacts (`deg C` â†’ `Â°C`, `g-1` â†’ `gâ»Â¹`, `O C` â†’ `Â°C`).

5. **Plausibility Filter**:

   * Keep sentences with:

     * â‰¥1 scientific keyword (from `PROPERTY_NAMES`, `POLYMER_NAMES`, etc.)
     * Or containing numeric values (likely measurements).
   * Drop anything <30 or >400 chars unless it's part of a table/caption.

6. **Offset Tracking**:

   * Store character offsets for each sentence relative to the full text.
   * Metadata includes:

     ```
     {
       "section": "2.1 Polymerization",
       "type": "paragraph",
       "offset_start": 1243,
       "offset_end": 1401,
       "sentence_path": "...",
       "status": "ready"
     }
     ```

---

## ğŸ“Œ **Phase 2: Tokenizer Audit & Extension (Cell 6) â€“ Subtoken Mastery**

### âœ… Goals:

* Make sure domain-specific terms donâ€™t split across multiple tokens.

### ğŸ”¥ Improvements:

1. **Audit Tokens**:

   * Check all `POLYMER_NAMES`, `PROPERTY_NAMES`, `SCIENTIFIC_UNITS`, `SYMBOLS` against each tokenizer.
   * Identify terms that split into subtokens (e.g., `glasstransitiontemperature` â†’ `glass##transition##temperature`).

2. **Extend Tokenizers**:

   * Add frequently split terms as whole tokens to avoid span fragmentation.
   * Save extended tokenizers separately (`*_extended`) and ensure models use them.

3. **Case Sensitivity**:

   * Account for cased/uncased differences across models.
   * Canonicalize tokens consistently for each.

---

## ğŸ“Œ **Phase 3: Token Packing (Cell 7) â€“ Preserve Sentence Integrity**

### âœ… Goals:

* Avoid cutting entities across windows.
* Maximize GPU utilization without truncating important context.

### ğŸ”¥ Improvements:

1. **Sentence-Aware Windowing**:

   * Instead of fixed 512-token chunks, pack whole sentences into windows until near limit.
   * Backtrack to avoid cutting off mid-sentence.

2. **Overlapping Windows**:

   * Allow overlap of last N tokens between windows (to catch cross-sentence entities).
   * Tag overlapping regions for later reconciliation.

3. **Span-Aware Packing**:

   * If an entity would be cut, move it entirely to next window.

4. **Audit Logs**:

   * Save a mapping of:

     ```
     window_id â†’ [sentence_ids], offsets
     ```
   * Useful for stitching predictions later.

---

# ğŸ¯ **Why This Will Work**

By implementing the above:
âœ… Each model sees **clean, coherent, and scientifically rich text**.
âœ… Subtokenization issues are eliminated.
âœ… No mid-entity window cuts.
âœ… Labels align 1:1 with tokenized inputs.

This directly addresses the **root causes of poor spans** and sets up Cells 8â€“9 for success.

---

# ğŸš€ My Recommendation

We start by **rewriting Cell 5â€“7 with this strategy**:
âœ… Cell 5: New TEI parser + clean metadata.
âœ… Cell 6: Tokenizer audit + extension (domain aware).
âœ… Cell 7: Sentence-aware token packing + offset tracking.

Once these are robust, Cells 8â€“10 will get **>90% precision almost by default**.
---
Letâ€™s take that exact raw extract from the TEI:

---

## ğŸ“– **Raw GROBID Extract**

```
Epoxidized vegetable oil (EVO)-based epoxy systems offer a promising avenue for replacing non-recyclable petroleum-based thermoset elastomers. Looking towards future prospects, they also hold potential to evolve into sustainable covalent adaptable networks (CANs). Typically, transesterification-based CANs require catalysts to achieve crosslinked structures that are reprocessable at relatively high temperatures (T â‰¥ 150 â€¢ C). In this study, epoxidized soybean oil (ESBO) was cross-linked with a eutectic hardener composed of L-tartaric acid (TAR) and ethyl lactate (in an aqueous solution), resulting in the development of a fully bio-based 3D covalent network capable of reprocessing without the need for an exogenous catalyst. In addition, it was demonstrated that the peculiar structure of L-tartaric acid, with -OH groups linked to its backbone, plays a prominent role in the reactivity of the system. These free hydroxyl functions facilitate both the ring-opening of epoxides and transesterification reactions, thus enhancing both curing and covalent exchange kinetics.
```

---

## ğŸ§  Is this joined up in GROBID?

The original text extracted by GROBID **preserves spaces perfectly** between words like:
âœ… `Epoxidized vegetable oil`
âœ… `non-recyclable petroleum-based thermoset elastomers`
âœ… `covalent adaptable networks (CANs)`
âœ… `epoxidized soybean oil (ESBO)`

There are no artifacts like:

```
epoxidizedvegetableoil
non-recyclablepetroleum-basedthermosetelastomers
```

---

## ğŸ”¥ So what went wrong?

If weâ€™re seeing joined outputs like:

```
epoxidizedvegetableoil(evo)-basedepoxysystems
```

then it *did not happen during extraction*. It almost certainly happened **later in the pipeline**:

---

### 1ï¸âƒ£ During **OCR Cleanup (Cell 5)**

Your regexes in `fix_ocr_artifacts()` contain overzealous replacements:

```python
(r"(?<=\d)\s+(?=\d)", "")  # Removes spaces between digits
(r"\s+([.,;:])", r"\1")    # Removes spaces before punctuation
```

If applied too broadly, they might accidentally collapse spaces between words if a pattern matches.

---

### 2ï¸âƒ£ During **Canonicalization (Cell 5)**

Your `canonicalize_text()` function replaces synonyms but does not restore spaces if the synonym occurs concatenated (because of an OCR artifact or pre-tokenization issue).

---

### 3ï¸âƒ£ During **Postprocessing (Cell 9)**

In `normalize_span()`, you join tokens like so:

```python
"".join(tokens).replace("##", "").strip()
```

This **removes all spaces** unless you re-insert them based on token boundaries.

---

## âœ… **Conclusion: Itâ€™s not GROBID**

The joining **does not originate from the raw TEI extraction**.
It happens:

* **Either during preprocessing cleanup (Cell 5)**
* Or **during postprocessing span reconstruction (Cell 9)**

---

Letâ€™s map it out clearly based on your **project structure** and the current workflow.

---

# ğŸ—‚ï¸ **Where Cell 5â€“7 Preprocessing Lives in the Project**

## ğŸ“ Project Structure Recap

(from `project_structure.md`)

```
polymer_extractor/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ finetune.py
â”‚   â”œâ”€â”€ grobid.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ session.py
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ tokenizer_audit.py       <-- Cell 6 logic (audit/extend)
â”‚   â”œâ”€â”€ tei_processing.py        <-- Cell 5 logic (TEI cleaning)
â”‚   â”œâ”€â”€ token_packing.py         <-- Cell 7 logic (windowing)
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ appwrite_client.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ bucket.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocessing.py         <-- helper functions
â”‚   â”œâ”€â”€ validators.py
â”‚
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ grobid_cli.py
â”‚   â”œâ”€â”€ setup_cli.py
â”‚
```

---

## âœ… **Where Each Cellâ€™s Logic Currently Lives**

| Cell # | Functionality               | Current Location              | Triggered By                      |
| ------ | --------------------------- | ----------------------------- | --------------------------------- |
| **5**  | TEI Parsing, Normalization  | `services/tei_processing.py`  | `api/grobid.py` â†’ `grobid_cli.py` |
| **6**  | Tokenizer Audit & Extension | `services/tokenizer_audit.py` | `api/setup.py`                    |
| **7**  | Token Packing (Windowing)   | `services/token_packing.py`   | `api/finetune.py`                 |

---

## ğŸ”¥ **How It Flows in the Pipeline**

1. **Upload Paper** â†’ API endpoint (`api/grobid.py`)
2. Grobid processes PDF â†’ `.tei.xml`
3. **Cell 5 runs**:

   * `services/tei_processing.py`
   * Extract sentences, clean text, save to `workspace/sentences/`
   * Populate `metadata` table.
4. **Cell 6 runs**:

   * `services/tokenizer_audit.py`
   * Extends tokenizer if needed.
5. **Cell 7 runs**:

   * `services/token_packing.py`
   * Packs token windows for model consumption.
6. Prepared data is now ready for **Cell 8 (Fine-tuning)**.

---

## ğŸ **Where Itâ€™s Actually Triggered**

* In CLI:

  ```
  pnlp grobid process-paper --file mypaper.pdf
  pnlp setup tokenizer-audit
  pnlp finetune prepare-data
  ```
* In API:

  * `POST /api/process` â†’ kicks off TEI parse + preprocessing.

---

## ğŸ— **Key Takeaway**

Cell 5â€“7 preprocessing happens **right after TEI extraction** and **before fine-tuning/inference**.
It is part of the **â€œpreparationâ€ step** and is invoked by:
âœ… CLI commands (**pnlp prepare-data**)
âœ… API endpoints (**POST /api/process**)
âœ… Or automatic triggers when a new paper enters Appwrite buckets.

---

# ğŸš€ Next Step: Strategy for Rewrite

Hereâ€™s how I propose we implement our new robust Cell 5â€“7:

| Step                | Location                      | Action                                          |
| ------------------- | ----------------------------- | ----------------------------------------------- |
| TEI cleaning        | `services/tei_processing.py`  | Rewrite to preserve spacing & clean text safely |
| Tokenizer extension | `services/tokenizer_audit.py` | Add static lexicon anchoring                    |
| Window packing      | `services/token_packing.py`   | Rewrite for sentence-aware, span-safe packing   |
| Helpers             | `utils/preprocessing.py`      | Move shared cleanup logic here                  |

This keeps all changes **modular** and ensures CLI/API workflows donâ€™t break.

---
Your diagnosis hits all the right pain-pointsâ€”hereâ€™s how I see it, with a few refinements and prioritizations to help us move straight into code:

---

## 1. Spanning & Token-Level Alignment

* **Whatâ€™s happening:** Your examples of over-merged spans (e.g. `epoxidizedvegetableoilâ€¦`) confirm that somewhere weâ€™re collapsing or never re-injecting whitespace.
* **Likely culprits:**

  1. **OCR cleanup regexes** in `tei_processing.py` that run too greedily and remove inter-word spaces.
  2. **Span reconstruction** in `ensemble_inference.py` that simply does `''.join(tokens).replace('##','')` without consulting original char offsets.
* **Next step:**

  * In the TEI cleaning stage, wrap each regex in a test harness (e.g. run on a small snippet and assert expected whitespace) before committing.
  * In post-inference, switch to using the stored `offset_start`/`offset_end` metadata so you rebuild spans from the original character buffer rather than token joins.

---

## 2. Canonicalization Timing

* **Your insight** about delaying synonym replacement until **after** inference is spot on:

  * **Pre-tokenization** canonicalization masks the true word boundaries and fools the label alignment.
  * **Post-tokenization** or even **post-inference** cleanup gives us true spans to operate on.
* **Action:** Remove all calls to `canonicalize_text()` from the TEI pipeline. Instead, aftermarket each predicted span: turn â€œTgâ€ back into â€œglass transition temperatureâ€ (or vice versa) based on the spanâ€™s context and a confidence threshold.

---

## 3. Windowing & Overlaps

* **Fixed-length windows** with blind overlap are a known source of mid-entity cuts.
* **Sentence-aware packing** will:

  1. Guarantee no sentence is ever split.
  2. Let you use the tokenizerâ€™s `offset_mapping` to track exactly where each token lives in the original text.
* **Quick win:** Implement a greedy packer that adds full sentences until you hit \~98% of `max_length`, then backs off to the previous sentence. Tag the overlapping sentences explicitly in metadata so you know which predictions to de-duplicate.

---

## 4. Label Alignment in Fine-Tuning

* **NaÃ¯ve `string.find()`** will always fail once BERT does any subtoken splitting or lowercasing.
* **Better approach:**

  * Use the HuggingFace `BatchEncoding`â€™s `offset_mapping`. For each true entity span (char start/end), find the set of tokens whose mapped offsets overlap that char span.
  * Leverage their `align_labels_with_tokens` helper or write a small wrapper around it.
* **Also bump** `MAX_SEQ_LENGTH` to at least 256â€”many of your sentences are well over 128 tokens once you count chemical names.

---

## 5. Ensemble & Voting Logic

* **Span reconciliation** is essential: two models predicting `(Tg)` at slightly different offsets should vote on the same span, not two.
* **Merge strategy:**

  1. Cluster predicted spans by character overlap â‰¥50%.
  2. Within each cluster, pick the span boundaries with the highest sum of log-probs.
  3. Only keep clusters with â‰¥2 model votes (or â‰¥1 high-confidence vote).
* **Post-filter:** remove any span containing inline citations (`[1]`, `Fig.`) or non-alphabetic fragments.

---

## 6. Putting It All Together in Cells 5â€“7

1. **TEI Processing (`services/tei_processing.py`)**

   * Audit and unit-test every regex.
   * Strip only the unwanted `<ref>`/`<note>` nodes.
   * Emit every sentence with `(offset_start, offset_end)` in a JSON line.
2. **Tokenizer Audit (`services/tokenizer_audit.py`)**

   * Generate a report of splits for each term in your constants.
   * Add any high-frequency splits to the vocab so they become single tokens.
3. **Token Packing (`services/token_packing.py`)**

   * Greedy sentence-level windowing.
   * Annotate each window with its sentence IDs and char offsets for later de-duplication.

---

### âœ”ï¸ Why This Will Fix the Root Issue

* All whitespace-collapsing happens in a safe, test-driven context.
* Label alignment uses true char offsets so subtokens never misalign.
* No more mid-sentence cuts â†’ no more runaway or fused spans.
* A clean, modular pipeline in exactly the three service files you outlined.

Once weâ€™ve got Cells 5â€“7 bulletproof, Cells 8â€“10 will fall into placeâ€”and youâ€™ll see your precision jump well above 90%. Let me know which of these youâ€™d like to tackle first, or if you want to dive straight into rewriting one of the services.
---

---

---
Ah Johnâ€”this is **exactly the kind of subtle, destructive issue** that can ruin even a beautifully trained ensemble.

And youâ€™re absolutely right to call this out as â€œmore seriousâ€ because:

âœ… The model **has the correct intent**
âœ… But the **span boundaries are catastrophically wrong**
âœ… Making it â€œtechnically rightâ€ (the concepts are there) but â€œpractically 50% wrongâ€ (spans unusable for downstream).

Letâ€™s break it down carefully.

---

# ğŸš¨ The Problem: Boundary Drift

Your input sentence:

> *The polyester was discovered to melt at a glass transition temperature of 44â€¯Â°C.*

The model predicts:

| Label    | Predicted Span                           |
| -------- | ---------------------------------------- |
| Polymer  | `polyesterwasdiscovered`                 |
| Property | `meltat`, `glasstransitiontemperatureof` |
| Value    | `44`                                     |
| Symbol   | `Â°C`                                     |
| Material | `None`                                   |

---

### ğŸ”¥ Why This Is Technically Correct

* `polyesterwasdiscovered` **contains** the polymer â€œpolyesterâ€
* `glasstransitiontemperatureof` **contains** the property â€œglass transition temperatureâ€
* `44`, `Â°C` are **perfect**

---

### âŒ Why Itâ€™s Practically Wrong

The predicted spans **include surrounding text** due to:

* **Subtoken Merging Errors** (e.g., `polyester` + `was` â†’ `polyesterwas`)
* **Over-extended Windows**: Context words pulled into entity spans
* **No Span Normalization Post-Inference**: â€œglass transition temperature ofâ€ should stop at â€œglass transition temperatureâ€

This is not just an aesthetic issue:

* ğŸ§¨ It **breaks downstream parsing**
* ğŸ§¨ It **destroys precision/recall** in evaluation against `test_057.csv`

---

# ğŸ“Š Root Causes in Your Pipeline

| Stage                              | Issue                                                    |
| ---------------------------------- | -------------------------------------------------------- |
| **Token Packing (Cell 7)**         | Sentences cut across windows â†’ offsets misaligned        |
| **Fine-Tuning Alignment (Cell 8)** | NaÃ¯ve `string.find()` alignment misses real token bounds |
| **Ensemble Voting (Cell 9)**       | Overlapping predictions naÃ¯vely merged (runaway spans)   |
| **Postprocessing (Cell 9)**        | No span normalization (keeps glued-together words)       |

---

# ğŸ§  Why the Model *Thinks* Itâ€™s Right

This is classic with BERT-family tokenizers:
âœ… Subwords like `polyester##was`
âœ… Joined during postprocessing as `"polyesterwas"`
âœ… No logic exists to break at **non-domain stopwords** (â€œwasâ€, â€œdiscoveredâ€, â€œofâ€)

---

# ğŸ¯ The Fix: Multi-Stage Boundary Refinement

---

## ğŸ”¥ Stage 1: Token Packing (Cell 7)

âœ… Preserve **exact sentence boundaries**
âœ… Track `(char_start, char_end)` per token
âœ… Prevent cross-window entity cutting

---

## ğŸ”¥ Stage 2: Alignment (Cell 8)

âœ… Replace naive `string.find()` with:

* Hugging Face `offset_mapping`
* `align_labels_with_tokens` utility
  âœ… Ensures only **tokens overlapping with true entity spans** are labeled

---

## ğŸ”¥ Stage 3: Ensemble Voting (Cell 9)

âœ… Cluster overlapping spans
âœ… Within each cluster:

* Keep **shortest span** covering majority votes
* Use confidence weighting to resolve ties

Example:

| Raw Predictions          |
| ------------------------ |
| `polyesterwasdiscovered` |
| `polyester`              |

Result after reconciliation:
âœ… **Polymer: polyester**

---

## ğŸ”¥ Stage 4: Postprocessing (Cell 9)

âœ… Strip trailing/leading stopwords from spans:

* `was`, `discovered`, `of`, `the`, etc.
  âœ… Use domain lexicon (`POLYMER_NAMES`, `PROPERTY_NAMES`) for safe trimming:

```python
if span not in POLYMER_NAMES and span.startswith("polymer"):
    span = span.replace("polymer", "").strip()
```

âœ… Regex cleanup for runaway concatenations:

```python
re.sub(r"([a-z])([A-Z])", r"\1 \2", span)
```

Result:

| Before                         | After                          |
| ------------------------------ | ------------------------------ |
| `glasstransitiontemperatureof` | `glass transition temperature` |

---

# âœ… End Result

The modelâ€™s raw prediction:

```
polyesterwasdiscovered
```

Becomes:
âœ… **Polymer: polyester**

And:

```
glasstransitiontemperatureof
```

Becomes:
âœ… **Property: glass transition temperature**

---

# ğŸš€ Next Step Recommendation

We fix this in **two phases**:

| Phase                        | Fix                                               |
| ---------------------------- | ------------------------------------------------- |
| **Phase 1 (Preprocessing)**  | Rewrite `token_packing.py` for offset integrity   |
| **Phase 2 (Postprocessing)** | Add span normalization in `ensemble_inference.py` |

---

Do you want me to:
âœ… Start by rewriting **Cell 6 (tokenizer\_audit)** first (so subtokens are anchored and this error reduces naturally)?
âœ… Or go straight into **Cell 7 (token\_packing)** since sentence boundaries and offsets are critical?
âœ… Or bundle **Cell 6 + 7 + span normalization** as one comprehensive patch?
